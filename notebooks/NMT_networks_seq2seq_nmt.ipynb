{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NMT networks_seq2seq_nmt.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marued/low-resource-machine-translation-team07/blob/master/notebooks/NMT_networks_seq2seq_nmt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bl9GdT7h0Hxk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhwgQAn50EZp",
        "colab_type": "text"
      },
      "source": [
        "# TensorFlow Addons Networks : Sequence-to-Sequence NMT with Attention Mechanism\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/addons/blob/master/docs/tutorials/networks_seq2seq_nmt.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/addons/blob/master/docs/tutorials/networks_seq2seq_nmt.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "      <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/addons/docs/tutorials/networks_seq2seq_nmt.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ip0n8178Fuwm",
        "colab_type": "text"
      },
      "source": [
        "## Overview\n",
        "This notebook gives a brief introduction into the ***Sequence to Sequence Model Architecture***\n",
        "In this noteboook we broadly cover four essential topics necessary for Neural Machine Translation:\n",
        "\n",
        "\n",
        "* **Data cleaning**\n",
        "* **Data preparation**\n",
        "* **Neural Translation Model with Attention**\n",
        "* **Final Translation**\n",
        "\n",
        "The basic idea behind such a model though, is only the encoder-decoder architecture. These networks are usually used for a variety of tasks like text-summerization, Machine translation, Image Captioning, etc. This tutorial provideas a hands-on understanding of the concept, explaining the technical jargons wherever necessary. We focus on the task of Neural Machine Translation (NMT) which was the very first testbed for seq2seq models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t58FF-i3_mFP",
        "colab_type": "code",
        "outputId": "ea92fb3d-e143-4718-c591-b81b1e2fa59c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Apr 13 06:09:27 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNiadLKNLleD",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bUHYPhlF-Ql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw044zGCZp-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1k17L6kgDi6",
        "colab_type": "code",
        "outputId": "d85d8230-4bd2-4849-eb91-82021c6c7563",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.85)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co6-YpBwL-4d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e7799c7a-c009-4298-d67f-4715371b1978"
      },
      "source": [
        "import csv\n",
        "import string\n",
        "import re\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "from numpy import array\n",
        "import itertools\n",
        "from pickle import load\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "import tensorflow as tf\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow_addons as tfa\n",
        "import numpy as np\n",
        "import sentencepiece as spm"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7gjUT_9XSoj",
        "colab_type": "text"
      },
      "source": [
        "## Data Cleaning\n",
        "\n",
        "Our data set is a German-English translation dataset. It contains 152,820 pairs of English to German phases, one pair per line with a tab separating the language. These dataset though organized needs cleaning before we can work on it. This will enable us to remove unnecessary bumps that may come in during the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwerJbyYpy8N",
        "colab_type": "code",
        "outputId": "bf18c1c3-98ed-4d7a-8257-4d108ea256e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYdvcZfNqr3F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os \n",
        "os.chdir(\"/content/drive/My Drive/2020Winter/IFT6759/project2/low-resource-machine-translation-team07/undreamt-tf/data/fr_cased_punc\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2bu44oBrVuq",
        "colab_type": "code",
        "outputId": "609ec89b-c0d1-4f41-95a2-bb0ff9dd5335",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "os.listdir()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['restult_dimension_300',\n",
              " 'sub_test.lang2.atok',\n",
              " 'sub_test.lang2',\n",
              " 'fr_bpe.vocab',\n",
              " 'test_fr_pred.txt.atok',\n",
              " 'en_bpe.vocab',\n",
              " 'en_bpe.model',\n",
              " 'fr_bpe.model',\n",
              " 'sub_test.lang1',\n",
              " 'test_fr_pred.txt',\n",
              " 'sub_test.lang1.atok',\n",
              " 'unaligned.cased.en',\n",
              " 'sub_train.lang2',\n",
              " 'unaligned.uncased.en',\n",
              " 'train.lang1',\n",
              " 'en_corpus.txt',\n",
              " 'unaligned.cased.fr',\n",
              " 'sub_train.lang1',\n",
              " 'sub_train.lang2.atok',\n",
              " 'unaligned.fr',\n",
              " 'sub_train.lang1.atok',\n",
              " 'train.lang2',\n",
              " 'fr_corpus.txt.atok',\n",
              " 'en_corpus.txt.atok',\n",
              " 'unaligned.en',\n",
              " 'fr_corpus.txt',\n",
              " 'unaligned.uncased.fr',\n",
              " 'multi-bleu-detok.perl',\n",
              " 'multi-bleu.perl',\n",
              " 'seq2seq_test_fr_pred.txt',\n",
              " 'sentencepiece_seq2seq_test_fr_pred.txt',\n",
              " 'nmt_networks_seq2seq_nmt.py']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cfb66QxWYr6A",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96JyJwWg6sei",
        "colab_type": "code",
        "outputId": "2f8f55f3-6337-42f9-c1e8-d613c3b236b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sp_en = spm.SentencePieceProcessor()\n",
        "sp_en.load('en_bpe.model')\n",
        "\n",
        "sp_fr = spm.SentencePieceProcessor()\n",
        "sp_fr.load('fr_bpe.model')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuumAqmMZDb2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_ids(sp, filename, start_tok=True, end_tok=True):\n",
        "    #<unk>=0, <s>=1, </s>=2\n",
        "    with open(filename, 'r') as f_in:\n",
        "        ids= list()\n",
        "        for line in f_in:\n",
        "            line= sp.encode_as_ids(line)\n",
        "            if start_tok: line.insert(0,1)\n",
        "            if end_tok: line.append(2)\n",
        "            ids.append(line)\n",
        "        return ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFWu1_nVuF0k",
        "colab_type": "code",
        "outputId": "22c2f4b6-a6b4-4c6a-8111-741cd6ec5b96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "data_en = load_ids(sp_en, \"sub_train.lang1\")\n",
        "print(data_en[0])\n",
        "print(sp_en.decode_ids(data_en[0]))\n",
        "\n",
        "data_fr = load_ids(sp_fr, \"sub_train.lang2\")\n",
        "print(data_fr[0])\n",
        "print(sp_fr.decode_ids(data_fr[0]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 144, 632, 560, 7, 1366, 52, 7024, 571, 3079, 3080, 47, 2753, 2]\n",
            "so too does the idea that accommodating religious differences is dangerous\n",
            "[1, 84, 1438, 1405, 12, 285, 3483, 49, 3821, 7297, 1354, 498, 7482, 17, 2]\n",
            "L ’ idée de concilier les différences religieuses semble donc dangereuse .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-4_g5mgSXfD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #<unk>=0, <s>=1, </s>=2, <sep>=3, <cls>=4\n",
        "# print(sp_fr.piece_to_id(\"<s>\"))\n",
        "# for id in range(4):\n",
        "#   print(sp_fr.id_to_piece(id), sp_fr.is_control(id))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oq60MBPSanQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_en = tf.keras.preprocessing.sequence.pad_sequences(data_en,padding='post')\n",
        "data_fr = tf.keras.preprocessing.sequence.pad_sequences(data_fr,padding='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beEvJIapW_ao",
        "colab_type": "code",
        "outputId": "c404d2ee-abea-4d9c-e783-32e39bb9ce51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i in range(5): print(data_en[:i]) "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n",
            "[[   1  144  632  560    7 1366   52 7024  571 3079 3080   47 2753    2\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0]]\n",
            "[[   1  144  632  560    7 1366   52 7024  571 3079 3080   47 2753    2\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [   1  192  234 1112   39 1114    7  700 2856  288 3424    7 2924   30\n",
            "     7   94  308 9965 2075  443 1846  519   87  536   87 3088    5 1273\n",
            "    66  474 2075   39 8878  259 1157  199  138   50    2    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0]]\n",
            "[[   1  144  632  560    7 1366   52 7024  571 3079 3080   47 2753    2\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [   1  192  234 1112   39 1114    7  700 2856  288 3424    7 2924   30\n",
            "     7   94  308 9965 2075  443 1846  519   87  536   87 3088    5 1273\n",
            "    66  474 2075   39 8878  259 1157  199  138   50    2    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [   1 4732  220 1567  439 1544   92  562 6725  444    2    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0]]\n",
            "[[   1  144  632  560    7 1366   52 7024  571 3079 3080   47 2753    2\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [   1  192  234 1112   39 1114    7  700 2856  288 3424    7 2924   30\n",
            "     7   94  308 9965 2075  443 1846  519   87  536   87 3088    5 1273\n",
            "    66  474 2075   39 8878  259 1157  199  138   50    2    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [   1 4732  220 1567  439 1544   92  562 6725  444    2    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [   1   60   47  248 5167 4213   52   57  171  371 4295  672  916   39\n",
            "  5332    2    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH5oSRNeSc1s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_len(tensor):\n",
        "    #print( np.argmax([len(t) for t in tensor]))\n",
        "    return max( len(t) for t in tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdM37lNBGXAj",
        "colab_type": "text"
      },
      "source": [
        "## Model Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfiBUJM2Et6C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X_train,  X_test, Y_train, Y_test = train_test_split(data_en,data_fr,test_size=0.2)\n",
        "X_train, Y_train = data_en, data_fr\n",
        "BATCH_SIZE = 128\n",
        "BUFFER_SIZE = len(X_train)\n",
        "steps_per_epoch = BUFFER_SIZE//BATCH_SIZE\n",
        "embedding_dims = 256\n",
        "rnn_units = 1024\n",
        "dense_units = 1024\n",
        "Dtype = tf.float32   #used to initialize DecoderCell Zero state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff_jQHLhGqJU",
        "colab_type": "text"
      },
      "source": [
        "## Dataset Prepration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b__1hPHVFALO",
        "colab_type": "code",
        "outputId": "bf5af970-e372-4ee1-98ec-0119930a8090",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "Tx = max_len(data_en)\n",
        "Ty = max_len(data_fr)  \n",
        "\n",
        "input_vocab_size = sp_en.get_piece_size()\n",
        "output_vocab_size = sp_fr.get_piece_size()\n",
        "\n",
        "print(input_vocab_size, output_vocab_size)\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "example_X, example_Y = next(iter(dataset))\n",
        "print(example_X.shape) \n",
        "print(example_Y.shape) "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000 10000\n",
            "(128, 111)\n",
            "(128, 159)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQRgJcYgapqE",
        "colab_type": "text"
      },
      "source": [
        "## Defining NMT Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGdakRtjaokF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/KonevDmitry/code_embeddings/blob/master/model/model.py\n",
        "\n",
        "#ENCODER\n",
        "class EncoderNetwork(tf.keras.Model):\n",
        "    def __init__(self,input_vocab_size,embedding_dims, rnn_units, bidirectional=True):\n",
        "        super().__init__()\n",
        "        self.encoder_embedding = tf.keras.layers.Embedding(input_dim=input_vocab_size,\n",
        "                                                           output_dim=embedding_dims)\n",
        "        self.encoder_rnnlayer = tf.keras.layers.LSTM(rnn_units,return_sequences=True, \n",
        "                                                     return_state=True )\n",
        "\n",
        "\n",
        "# class Encoder(tf.keras.Model):\n",
        "#     def __init__(self, input_vocab_size, embedding_dims, rnn_units, batch_sz):\n",
        "#         super(Encoder, self).__init__()\n",
        "#         self.batch_sz = batch_sz\n",
        "#         self.rnn_units = rnn_units\n",
        "#         self.encoder_embedding = tf.keras.layers.Embedding(input_vocab_size, embedding_dims)\n",
        "#         self.lstm = tf.keras.layers.LSTM(self.rnn_units,\n",
        "#                                        return_sequences=True,\n",
        "#                                        return_state=True,\n",
        "#                                        recurrent_initializer='glorot_uniform')\n",
        "#         self.encoder_rnnlayer = tf.keras.layers.Bidirectional(self.lstm)\n",
        "\n",
        "#     def call(self, x, initial_state):\n",
        "#         x = self.encoder_embedding(x)\n",
        "#         output, stateF, stateB = self.encoder_rnnlayer(x, initial_state = initial_state)\n",
        "#         return output, [stateF, stateB]\n",
        "\n",
        "#     def initialize_hidden_state(self):\n",
        "#         init_state = [tf.zeros((self.batch_sz, self.rnn_units)) for i in range(2)]\n",
        "#         return init_state\n",
        "    \n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz=32):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        gru = tf.keras.layers.LSTM(self.enc_units // 2,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "        self.bidi = tf.keras.layers.Bidirectional(gru, merge_mode='concat')\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output = self.bidi(x)\n",
        "\n",
        "        whole_sequence_output=output[0]\n",
        "        final_memory_state=tf.concat([output[1], output[3]], axis=1)\n",
        "        final_carry_state=tf.concat([output[2], output[4]], axis=1)\n",
        "\n",
        "        return whole_sequence_output, final_memory_state, final_carry_state\n",
        "        # return output, stateF, stateB\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        init_state = [tf.zeros((self.batch_sz, self.enc_units //2 )) for i in range(2)]\n",
        "        \n",
        "        return init_state\n",
        "\n",
        "#DECODER\n",
        "class DecoderNetwork(tf.keras.Model):\n",
        "    def __init__(self,output_vocab_size, embedding_dims, rnn_units):\n",
        "        super().__init__()\n",
        "        self.decoder_embedding = tf.keras.layers.Embedding(input_dim=output_vocab_size,\n",
        "                                                           output_dim=embedding_dims) \n",
        "        self.dense_layer = tf.keras.layers.Dense(output_vocab_size)\n",
        "        self.decoder_rnncell = tf.keras.layers.LSTMCell(rnn_units)\n",
        "        # Sampler\n",
        "        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
        "        # Create attention mechanism with memory = None\n",
        "        self.attention_mechanism = self.build_attention_mechanism(dense_units,None,BATCH_SIZE*[Tx])\n",
        "        self.rnn_cell =  self.build_rnn_cell(BATCH_SIZE)\n",
        "        self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler= self.sampler,\n",
        "                                                output_layer=self.dense_layer)\n",
        "\n",
        "    def build_attention_mechanism(self, units,memory, memory_sequence_length):\n",
        "        return tfa.seq2seq.LuongAttention(units, memory = memory, \n",
        "                                          memory_sequence_length=memory_sequence_length)\n",
        "        #return tfa.seq2seq.BahdanauAttention(units, memory = memory, memory_sequence_length=memory_sequence_length)\n",
        "\n",
        "    # wrap decodernn cell  \n",
        "    def build_rnn_cell(self, batch_size ):\n",
        "        rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnncell, self.attention_mechanism,\n",
        "                                                attention_layer_size=dense_units)\n",
        "        return rnn_cell\n",
        "    \n",
        "    def build_decoder_initial_state(self, batch_size, encoder_state,Dtype):\n",
        "        decoder_initial_state = self.rnn_cell.get_initial_state(batch_size = batch_size, \n",
        "                                                                dtype = Dtype)\n",
        "        decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state) \n",
        "        return decoder_initial_state\n",
        "\n",
        "# encoderNetwork = EncoderNetwork(input_vocab_size,embedding_dims, rnn_units)\n",
        "encoderNetwork = Encoder(input_vocab_size,embedding_dims, rnn_units, BATCH_SIZE)\n",
        "decoderNetwork = DecoderNetwork(output_vocab_size,embedding_dims, rnn_units)\n",
        "optimizer = tf.keras.optimizers.Adam()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPwcfddTa0oB",
        "colab_type": "text"
      },
      "source": [
        "## Initializing Training functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1BEqVyra2jW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_function(y_pred, y):\n",
        "   \n",
        "    #shape of y [batch_size, ty]\n",
        "    #shape of y_pred [batch_size, Ty, output_vocab_size] \n",
        "    sparsecategoricalcrossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                                                  reduction='none')\n",
        "    loss = sparsecategoricalcrossentropy(y_true=y, y_pred=y_pred)\n",
        "    mask = tf.logical_not(tf.math.equal(y,0))   #output 0 for y=0 else output 1\n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "    loss = mask* loss\n",
        "    loss = tf.reduce_mean(loss)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def train_step(input_batch, output_batch,encoder_initial_cell_state):\n",
        "    #initialize loss = 0\n",
        "    loss = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "        # encoder_emb_inp = encoderNetwork.encoder_embedding(input_batch)\n",
        "\n",
        "        # a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp, \n",
        "        #                                                 initial_state =encoder_initial_cell_state)\n",
        "        a, a_tx, c_tx = encoderNetwork(input_batch, encoder_initial_cell_state)\n",
        "\n",
        "        #[last step activations,last memory_state] of encoder passed as input to decoder Network\n",
        "        \n",
        "         \n",
        "        # Prepare correct Decoder input & output sequence data\n",
        "        decoder_input = output_batch[:,:-1] # ignore <end>\n",
        "        #compare logits with timestepped +1 version of decoder_input\n",
        "        decoder_output = output_batch[:,1:] #ignore <start>\n",
        "\n",
        "\n",
        "        # Decoder Embeddings\n",
        "        decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
        "\n",
        "        #Setting up decoder memory from encoder output and Zero State for AttentionWrapperState\n",
        "        decoderNetwork.attention_mechanism.setup_memory(a)\n",
        "        decoder_initial_state = decoderNetwork.build_decoder_initial_state(BATCH_SIZE,\n",
        "                                                                           encoder_state=[a_tx, c_tx],\n",
        "                                                                           Dtype=tf.float32)\n",
        "        \n",
        "        #BasicDecoderOutput        \n",
        "        outputs, _, _ = decoderNetwork.decoder(decoder_emb_inp,initial_state=decoder_initial_state,\n",
        "                                               sequence_length=BATCH_SIZE*[Ty-1])\n",
        "\n",
        "        logits = outputs.rnn_output\n",
        "        #Calculate loss\n",
        "\n",
        "        loss = loss_function(logits, decoder_output)\n",
        "\n",
        "    #Returns the list of all layer variables / weights.\n",
        "    variables = encoderNetwork.trainable_variables + decoderNetwork.trainable_variables  \n",
        "    # differentiate loss wrt variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    #grads_and_vars – List of(gradient, variable) pairs.\n",
        "    grads_and_vars = zip(gradients,variables)\n",
        "    optimizer.apply_gradients(grads_and_vars)\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71Lkdx6GFb3A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#RNN LSTM hidden and memory state initializer\n",
        "def initialize_initial_state():\n",
        "    # [num_of layers, batch, hidden]\n",
        "    return [tf.zeros((BATCH_SIZE, rnn_units)), tf.zeros((BATCH_SIZE, rnn_units))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5uzLcu2bNX3",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_NPhHNtKAxG",
        "colab_type": "text"
      },
      "source": [
        "Load checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckyqY561FjEj",
        "colab_type": "code",
        "outputId": "ccbf48a3-184a-4cff-fdcc-524a89efc51f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# https://github.com/dhirensk/ai/blob/master/practice/english_to_french_seq2seq_tf_2_0_withattention.py\n",
        "\n",
        "checkpointdir = os.path.join('/content/drive/My Drive/2020Winter/IFT6759/project2/low-resource-machine-translation-team07/',\"test\")\n",
        "chkpoint_prefix = os.path.join(checkpointdir, \"chkpoint\")\n",
        "if not os.path.exists(checkpointdir):\n",
        "    os.mkdir(checkpointdir)\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(optimizer = optimizer, encoderNetwork = encoderNetwork, \n",
        "                                 decoderNetwork = decoderNetwork)\n",
        "\n",
        "try:\n",
        "    status = checkpoint.restore(tf.train.latest_checkpoint(checkpointdir))\n",
        "    print(\"Checkpoint found at {}\".format(tf.train.latest_checkpoint(checkpointdir)))\n",
        "except:\n",
        "    print(\"No checkpoint found at {}\".format(checkpointdir))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checkpoint found at None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZjDz4oCKEEh",
        "colab_type": "text"
      },
      "source": [
        "Train steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvfD2SknWrt6",
        "colab_type": "code",
        "outputId": "e7a50c51-e46c-4660-aa5e-1028723ea976",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epochs = 30\n",
        "for i in range(1, epochs+1):\n",
        "\n",
        "    # encoder_initial_cell_state = initialize_initial_state()\n",
        "    encoder_initial_cell_state = encoderNetwork.initialize_hidden_state()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for ( batch , (input_batch, output_batch)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(input_batch, output_batch, encoder_initial_cell_state)\n",
        "        total_loss += batch_loss\n",
        "        if (batch+1)%5 == 0:\n",
        "            print(\"total loss: {} epoch {} batch {} \".format(batch_loss.numpy(), i, batch+1))\n",
        "    checkpoint.save(file_prefix = chkpoint_prefix)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total loss: 1.6595466136932373 epoch 1 batch 5 \n",
            "total loss: 1.3233318328857422 epoch 1 batch 10 \n",
            "total loss: 1.2915977239608765 epoch 1 batch 15 \n",
            "total loss: 1.1823575496673584 epoch 1 batch 20 \n",
            "total loss: 1.2220191955566406 epoch 1 batch 25 \n",
            "total loss: 1.141091227531433 epoch 1 batch 30 \n",
            "total loss: 1.259871482849121 epoch 1 batch 35 \n",
            "total loss: 1.1029536724090576 epoch 1 batch 40 \n",
            "total loss: 1.1422545909881592 epoch 1 batch 45 \n",
            "total loss: 1.185848593711853 epoch 1 batch 50 \n",
            "total loss: 1.0235154628753662 epoch 1 batch 55 \n",
            "total loss: 1.2659356594085693 epoch 1 batch 60 \n",
            "total loss: 1.1213037967681885 epoch 1 batch 65 \n",
            "total loss: 1.1102582216262817 epoch 1 batch 70 \n",
            "total loss: 1.0557441711425781 epoch 1 batch 75 \n",
            "total loss: 1.1541165113449097 epoch 2 batch 5 \n",
            "total loss: 1.0210813283920288 epoch 2 batch 10 \n",
            "total loss: 1.0645779371261597 epoch 2 batch 15 \n",
            "total loss: 1.1152064800262451 epoch 2 batch 20 \n",
            "total loss: 1.1407300233840942 epoch 2 batch 25 \n",
            "total loss: 1.062313437461853 epoch 2 batch 30 \n",
            "total loss: 0.9838127493858337 epoch 2 batch 35 \n",
            "total loss: 1.0250890254974365 epoch 2 batch 40 \n",
            "total loss: 1.0199204683303833 epoch 2 batch 45 \n",
            "total loss: 1.0782963037490845 epoch 2 batch 50 \n",
            "total loss: 0.8985673189163208 epoch 2 batch 55 \n",
            "total loss: 1.011788249015808 epoch 2 batch 60 \n",
            "total loss: 1.062557339668274 epoch 2 batch 65 \n",
            "total loss: 0.9244046807289124 epoch 2 batch 70 \n",
            "total loss: 0.9522430896759033 epoch 2 batch 75 \n",
            "total loss: 0.9932372570037842 epoch 3 batch 5 \n",
            "total loss: 0.9977203607559204 epoch 3 batch 10 \n",
            "total loss: 1.0274161100387573 epoch 3 batch 15 \n",
            "total loss: 1.0485299825668335 epoch 3 batch 20 \n",
            "total loss: 0.9038259983062744 epoch 3 batch 25 \n",
            "total loss: 0.9522407650947571 epoch 3 batch 30 \n",
            "total loss: 0.8570751547813416 epoch 3 batch 35 \n",
            "total loss: 0.9128735661506653 epoch 3 batch 40 \n",
            "total loss: 0.9634897708892822 epoch 3 batch 45 \n",
            "total loss: 1.0267870426177979 epoch 3 batch 50 \n",
            "total loss: 0.9937261343002319 epoch 3 batch 55 \n",
            "total loss: 0.9988530874252319 epoch 3 batch 60 \n",
            "total loss: 0.9956635236740112 epoch 3 batch 65 \n",
            "total loss: 0.9553037285804749 epoch 3 batch 70 \n",
            "total loss: 0.9354135990142822 epoch 3 batch 75 \n",
            "total loss: 0.9567641019821167 epoch 4 batch 5 \n",
            "total loss: 0.9443767070770264 epoch 4 batch 10 \n",
            "total loss: 0.9199008345603943 epoch 4 batch 15 \n",
            "total loss: 0.919816792011261 epoch 4 batch 20 \n",
            "total loss: 0.85965895652771 epoch 4 batch 25 \n",
            "total loss: 0.8312699794769287 epoch 4 batch 30 \n",
            "total loss: 0.8802496790885925 epoch 4 batch 35 \n",
            "total loss: 0.8633424639701843 epoch 4 batch 40 \n",
            "total loss: 0.8111038208007812 epoch 4 batch 45 \n",
            "total loss: 0.9170312285423279 epoch 4 batch 50 \n",
            "total loss: 0.835277795791626 epoch 4 batch 55 \n",
            "total loss: 0.8550878763198853 epoch 4 batch 60 \n",
            "total loss: 0.8259029388427734 epoch 4 batch 65 \n",
            "total loss: 0.9101555943489075 epoch 4 batch 70 \n",
            "total loss: 0.893439769744873 epoch 4 batch 75 \n",
            "total loss: 0.8968524932861328 epoch 5 batch 5 \n",
            "total loss: 0.8296900987625122 epoch 5 batch 10 \n",
            "total loss: 0.7685765027999878 epoch 5 batch 15 \n",
            "total loss: 0.7787159085273743 epoch 5 batch 20 \n",
            "total loss: 0.8316841721534729 epoch 5 batch 25 \n",
            "total loss: 0.8330182433128357 epoch 5 batch 30 \n",
            "total loss: 0.8581704497337341 epoch 5 batch 35 \n",
            "total loss: 0.899578332901001 epoch 5 batch 40 \n",
            "total loss: 0.8528023362159729 epoch 5 batch 45 \n",
            "total loss: 0.8762102723121643 epoch 5 batch 50 \n",
            "total loss: 0.7852399945259094 epoch 5 batch 55 \n",
            "total loss: 0.7871503233909607 epoch 5 batch 60 \n",
            "total loss: 0.8603401780128479 epoch 5 batch 65 \n",
            "total loss: 0.763349711894989 epoch 5 batch 70 \n",
            "total loss: 0.8223276138305664 epoch 5 batch 75 \n",
            "total loss: 0.8479567170143127 epoch 6 batch 5 \n",
            "total loss: 0.7129949927330017 epoch 6 batch 10 \n",
            "total loss: 0.6691219806671143 epoch 6 batch 15 \n",
            "total loss: 0.7733900547027588 epoch 6 batch 20 \n",
            "total loss: 0.7570721507072449 epoch 6 batch 25 \n",
            "total loss: 0.8258726596832275 epoch 6 batch 30 \n",
            "total loss: 0.7977313995361328 epoch 6 batch 35 \n",
            "total loss: 0.7381478548049927 epoch 6 batch 40 \n",
            "total loss: 0.7850934267044067 epoch 6 batch 45 \n",
            "total loss: 0.788309633731842 epoch 6 batch 50 \n",
            "total loss: 0.7714288830757141 epoch 6 batch 55 \n",
            "total loss: 0.7566834688186646 epoch 6 batch 60 \n",
            "total loss: 0.8240709900856018 epoch 6 batch 65 \n",
            "total loss: 0.7728926539421082 epoch 6 batch 70 \n",
            "total loss: 0.7765076756477356 epoch 6 batch 75 \n",
            "total loss: 0.7297417521476746 epoch 7 batch 5 \n",
            "total loss: 0.6801496148109436 epoch 7 batch 10 \n",
            "total loss: 0.7454768419265747 epoch 7 batch 15 \n",
            "total loss: 0.7494654059410095 epoch 7 batch 20 \n",
            "total loss: 0.6655300259590149 epoch 7 batch 25 \n",
            "total loss: 0.6994013786315918 epoch 7 batch 30 \n",
            "total loss: 0.7551648616790771 epoch 7 batch 35 \n",
            "total loss: 0.6804559230804443 epoch 7 batch 40 \n",
            "total loss: 0.7174674272537231 epoch 7 batch 45 \n",
            "total loss: 0.7021097540855408 epoch 7 batch 50 \n",
            "total loss: 0.6797095537185669 epoch 7 batch 55 \n",
            "total loss: 0.7431399822235107 epoch 7 batch 60 \n",
            "total loss: 0.7058646082878113 epoch 7 batch 65 \n",
            "total loss: 0.6975036859512329 epoch 7 batch 70 \n",
            "total loss: 0.7581638693809509 epoch 7 batch 75 \n",
            "total loss: 0.637391209602356 epoch 8 batch 5 \n",
            "total loss: 0.6318226456642151 epoch 8 batch 10 \n",
            "total loss: 0.6462273001670837 epoch 8 batch 15 \n",
            "total loss: 0.6539426445960999 epoch 8 batch 20 \n",
            "total loss: 0.6627412438392639 epoch 8 batch 25 \n",
            "total loss: 0.6400668025016785 epoch 8 batch 30 \n",
            "total loss: 0.5984878540039062 epoch 8 batch 35 \n",
            "total loss: 0.7318801283836365 epoch 8 batch 40 \n",
            "total loss: 0.6797769069671631 epoch 8 batch 45 \n",
            "total loss: 0.6057644486427307 epoch 8 batch 50 \n",
            "total loss: 0.6333776712417603 epoch 8 batch 55 \n",
            "total loss: 0.6826874017715454 epoch 8 batch 60 \n",
            "total loss: 0.6313836574554443 epoch 8 batch 65 \n",
            "total loss: 0.6928510069847107 epoch 8 batch 70 \n",
            "total loss: 0.6578171849250793 epoch 8 batch 75 \n",
            "total loss: 0.626050591468811 epoch 9 batch 5 \n",
            "total loss: 0.5581652522087097 epoch 9 batch 10 \n",
            "total loss: 0.5426589250564575 epoch 9 batch 15 \n",
            "total loss: 0.5664821863174438 epoch 9 batch 20 \n",
            "total loss: 0.6030169725418091 epoch 9 batch 25 \n",
            "total loss: 0.6311094760894775 epoch 9 batch 30 \n",
            "total loss: 0.6047692894935608 epoch 9 batch 35 \n",
            "total loss: 0.5920630097389221 epoch 9 batch 40 \n",
            "total loss: 0.6202296018600464 epoch 9 batch 45 \n",
            "total loss: 0.5367491245269775 epoch 9 batch 50 \n",
            "total loss: 0.6456308364868164 epoch 9 batch 55 \n",
            "total loss: 0.6584523320198059 epoch 9 batch 60 \n",
            "total loss: 0.5964874029159546 epoch 9 batch 65 \n",
            "total loss: 0.6516649127006531 epoch 9 batch 70 \n",
            "total loss: 0.6933033466339111 epoch 9 batch 75 \n",
            "total loss: 0.5252209901809692 epoch 10 batch 5 \n",
            "total loss: 0.5424944758415222 epoch 10 batch 10 \n",
            "total loss: 0.6416246294975281 epoch 10 batch 15 \n",
            "total loss: 0.5111872553825378 epoch 10 batch 20 \n",
            "total loss: 0.5583339929580688 epoch 10 batch 25 \n",
            "total loss: 0.5663936734199524 epoch 10 batch 30 \n",
            "total loss: 0.5412520170211792 epoch 10 batch 35 \n",
            "total loss: 0.5546729564666748 epoch 10 batch 40 \n",
            "total loss: 0.5829303860664368 epoch 10 batch 45 \n",
            "total loss: 0.6153814792633057 epoch 10 batch 50 \n",
            "total loss: 0.5619809031486511 epoch 10 batch 55 \n",
            "total loss: 0.6049202084541321 epoch 10 batch 60 \n",
            "total loss: 0.5968520641326904 epoch 10 batch 65 \n",
            "total loss: 0.5768040418624878 epoch 10 batch 70 \n",
            "total loss: 0.5835320949554443 epoch 10 batch 75 \n",
            "total loss: 0.5305604338645935 epoch 11 batch 5 \n",
            "total loss: 0.5185965299606323 epoch 11 batch 10 \n",
            "total loss: 0.5205010771751404 epoch 11 batch 15 \n",
            "total loss: 0.5008376240730286 epoch 11 batch 20 \n",
            "total loss: 0.45556506514549255 epoch 11 batch 25 \n",
            "total loss: 0.4919428825378418 epoch 11 batch 30 \n",
            "total loss: 0.5093798041343689 epoch 11 batch 35 \n",
            "total loss: 0.4880380630493164 epoch 11 batch 40 \n",
            "total loss: 0.5563042759895325 epoch 11 batch 45 \n",
            "total loss: 0.5228670835494995 epoch 11 batch 50 \n",
            "total loss: 0.5377793312072754 epoch 11 batch 55 \n",
            "total loss: 0.5319916009902954 epoch 11 batch 60 \n",
            "total loss: 0.5390316247940063 epoch 11 batch 65 \n",
            "total loss: 0.5280523300170898 epoch 11 batch 70 \n",
            "total loss: 0.5186524987220764 epoch 11 batch 75 \n",
            "total loss: 0.43763646483421326 epoch 12 batch 5 \n",
            "total loss: 0.44442376494407654 epoch 12 batch 10 \n",
            "total loss: 0.42459380626678467 epoch 12 batch 15 \n",
            "total loss: 0.4121507704257965 epoch 12 batch 20 \n",
            "total loss: 0.42584335803985596 epoch 12 batch 25 \n",
            "total loss: 0.4505222737789154 epoch 12 batch 30 \n",
            "total loss: 0.43514788150787354 epoch 12 batch 35 \n",
            "total loss: 0.49052754044532776 epoch 12 batch 40 \n",
            "total loss: 0.43587079644203186 epoch 12 batch 45 \n",
            "total loss: 0.4454042911529541 epoch 12 batch 50 \n",
            "total loss: 0.49555596709251404 epoch 12 batch 55 \n",
            "total loss: 0.5063409209251404 epoch 12 batch 60 \n",
            "total loss: 0.5130217671394348 epoch 12 batch 65 \n",
            "total loss: 0.5378379225730896 epoch 12 batch 70 \n",
            "total loss: 0.4882793128490448 epoch 12 batch 75 \n",
            "total loss: 0.3892708122730255 epoch 13 batch 5 \n",
            "total loss: 0.3768485486507416 epoch 13 batch 10 \n",
            "total loss: 0.36997953057289124 epoch 13 batch 15 \n",
            "total loss: 0.41681569814682007 epoch 13 batch 20 \n",
            "total loss: 0.43693143129348755 epoch 13 batch 25 \n",
            "total loss: 0.4321689307689667 epoch 13 batch 30 \n",
            "total loss: 0.4134518504142761 epoch 13 batch 35 \n",
            "total loss: 0.4681592583656311 epoch 13 batch 40 \n",
            "total loss: 0.3998960554599762 epoch 13 batch 45 \n",
            "total loss: 0.4104115962982178 epoch 13 batch 50 \n",
            "total loss: 0.4559920132160187 epoch 13 batch 55 \n",
            "total loss: 0.4197464883327484 epoch 13 batch 60 \n",
            "total loss: 0.4095618426799774 epoch 13 batch 65 \n",
            "total loss: 0.44746047258377075 epoch 13 batch 70 \n",
            "total loss: 0.4164732098579407 epoch 13 batch 75 \n",
            "total loss: 0.3171570599079132 epoch 14 batch 5 \n",
            "total loss: 0.3660595118999481 epoch 14 batch 10 \n",
            "total loss: 0.3524157404899597 epoch 14 batch 15 \n",
            "total loss: 0.3556362986564636 epoch 14 batch 20 \n",
            "total loss: 0.4301246106624603 epoch 14 batch 25 \n",
            "total loss: 0.41103246808052063 epoch 14 batch 30 \n",
            "total loss: 0.40995103120803833 epoch 14 batch 35 \n",
            "total loss: 0.38714006543159485 epoch 14 batch 40 \n",
            "total loss: 0.4002281427383423 epoch 14 batch 45 \n",
            "total loss: 0.4211844801902771 epoch 14 batch 50 \n",
            "total loss: 0.41699329018592834 epoch 14 batch 55 \n",
            "total loss: 0.4738081097602844 epoch 14 batch 60 \n",
            "total loss: 0.3976765275001526 epoch 14 batch 65 \n",
            "total loss: 0.39324232935905457 epoch 14 batch 70 \n",
            "total loss: 0.4137389659881592 epoch 14 batch 75 \n",
            "total loss: 0.3227454423904419 epoch 15 batch 5 \n",
            "total loss: 0.31662455201148987 epoch 15 batch 10 \n",
            "total loss: 0.3409217596054077 epoch 15 batch 15 \n",
            "total loss: 0.36210235953330994 epoch 15 batch 20 \n",
            "total loss: 0.33110517263412476 epoch 15 batch 25 \n",
            "total loss: 0.31964927911758423 epoch 15 batch 30 \n",
            "total loss: 0.33406880497932434 epoch 15 batch 35 \n",
            "total loss: 0.3189496397972107 epoch 15 batch 40 \n",
            "total loss: 0.3285474181175232 epoch 15 batch 45 \n",
            "total loss: 0.35224977135658264 epoch 15 batch 50 \n",
            "total loss: 0.3582172095775604 epoch 15 batch 55 \n",
            "total loss: 0.38328152894973755 epoch 15 batch 60 \n",
            "total loss: 0.39785245060920715 epoch 15 batch 65 \n",
            "total loss: 0.36938244104385376 epoch 15 batch 70 \n",
            "total loss: 0.39115485548973083 epoch 15 batch 75 \n",
            "total loss: 0.30320364236831665 epoch 16 batch 5 \n",
            "total loss: 0.29656967520713806 epoch 16 batch 10 \n",
            "total loss: 0.2670675814151764 epoch 16 batch 15 \n",
            "total loss: 0.2984434962272644 epoch 16 batch 20 \n",
            "total loss: 0.2728951871395111 epoch 16 batch 25 \n",
            "total loss: 0.2967049777507782 epoch 16 batch 30 \n",
            "total loss: 0.3197161555290222 epoch 16 batch 35 \n",
            "total loss: 0.3038656413555145 epoch 16 batch 40 \n",
            "total loss: 0.29539546370506287 epoch 16 batch 45 \n",
            "total loss: 0.3135036826133728 epoch 16 batch 50 \n",
            "total loss: 0.34278199076652527 epoch 16 batch 55 \n",
            "total loss: 0.33714932203292847 epoch 16 batch 60 \n",
            "total loss: 0.3057559132575989 epoch 16 batch 65 \n",
            "total loss: 0.3347892761230469 epoch 16 batch 70 \n",
            "total loss: 0.3236875832080841 epoch 16 batch 75 \n",
            "total loss: 0.2740194499492645 epoch 17 batch 5 \n",
            "total loss: 0.2087528556585312 epoch 17 batch 10 \n",
            "total loss: 0.2517559826374054 epoch 17 batch 15 \n",
            "total loss: 0.24284031987190247 epoch 17 batch 20 \n",
            "total loss: 0.28050288558006287 epoch 17 batch 25 \n",
            "total loss: 0.2917044162750244 epoch 17 batch 30 \n",
            "total loss: 0.263751357793808 epoch 17 batch 35 \n",
            "total loss: 0.2885761559009552 epoch 17 batch 40 \n",
            "total loss: 0.2775138020515442 epoch 17 batch 45 \n",
            "total loss: 0.2929920256137848 epoch 17 batch 50 \n",
            "total loss: 0.3019895553588867 epoch 17 batch 55 \n",
            "total loss: 0.28146758675575256 epoch 17 batch 60 \n",
            "total loss: 0.2783348560333252 epoch 17 batch 65 \n",
            "total loss: 0.31468117237091064 epoch 17 batch 70 \n",
            "total loss: 0.32094642519950867 epoch 17 batch 75 \n",
            "total loss: 0.2067584991455078 epoch 18 batch 5 \n",
            "total loss: 0.23473693430423737 epoch 18 batch 10 \n",
            "total loss: 0.22643975913524628 epoch 18 batch 15 \n",
            "total loss: 0.23131780326366425 epoch 18 batch 20 \n",
            "total loss: 0.24353553354740143 epoch 18 batch 25 \n",
            "total loss: 0.22620368003845215 epoch 18 batch 30 \n",
            "total loss: 0.2482379525899887 epoch 18 batch 35 \n",
            "total loss: 0.2341790497303009 epoch 18 batch 40 \n",
            "total loss: 0.25204038619995117 epoch 18 batch 45 \n",
            "total loss: 0.2797829210758209 epoch 18 batch 50 \n",
            "total loss: 0.2539267838001251 epoch 18 batch 55 \n",
            "total loss: 0.2686828076839447 epoch 18 batch 60 \n",
            "total loss: 0.2761858105659485 epoch 18 batch 65 \n",
            "total loss: 0.2783563733100891 epoch 18 batch 70 \n",
            "total loss: 0.28161996603012085 epoch 18 batch 75 \n",
            "total loss: 0.2138899266719818 epoch 19 batch 5 \n",
            "total loss: 0.21781593561172485 epoch 19 batch 10 \n",
            "total loss: 0.22132927179336548 epoch 19 batch 20 \n",
            "total loss: 0.21455803513526917 epoch 19 batch 25 \n",
            "total loss: 0.20481795072555542 epoch 19 batch 30 \n",
            "total loss: 0.23785975575447083 epoch 19 batch 35 \n",
            "total loss: 0.23558320105075836 epoch 19 batch 40 \n",
            "total loss: 0.22679726779460907 epoch 19 batch 45 \n",
            "total loss: 0.22791826725006104 epoch 19 batch 50 \n",
            "total loss: 0.2760552763938904 epoch 19 batch 55 \n",
            "total loss: 0.2504059076309204 epoch 19 batch 60 \n",
            "total loss: 0.26651236414909363 epoch 19 batch 65 \n",
            "total loss: 0.27171799540519714 epoch 19 batch 70 \n",
            "total loss: 0.2664925158023834 epoch 19 batch 75 \n",
            "total loss: 0.1971479058265686 epoch 20 batch 5 \n",
            "total loss: 0.21228229999542236 epoch 20 batch 10 \n",
            "total loss: 0.18845240771770477 epoch 20 batch 15 \n",
            "total loss: 0.2139519304037094 epoch 20 batch 20 \n",
            "total loss: 0.21707192063331604 epoch 20 batch 25 \n",
            "total loss: 0.227138951420784 epoch 20 batch 30 \n",
            "total loss: 0.22304078936576843 epoch 20 batch 35 \n",
            "total loss: 0.21475352346897125 epoch 20 batch 40 \n",
            "total loss: 0.1972804218530655 epoch 20 batch 45 \n",
            "total loss: 0.20897167921066284 epoch 20 batch 50 \n",
            "total loss: 0.24054324626922607 epoch 20 batch 55 \n",
            "total loss: 0.2366294115781784 epoch 20 batch 60 \n",
            "total loss: 0.22255651652812958 epoch 20 batch 65 \n",
            "total loss: 0.22479233145713806 epoch 20 batch 70 \n",
            "total loss: 0.22543175518512726 epoch 20 batch 75 \n",
            "total loss: 0.17509010434150696 epoch 21 batch 5 \n",
            "total loss: 0.19774870574474335 epoch 21 batch 10 \n",
            "total loss: 0.19750681519508362 epoch 21 batch 15 \n",
            "total loss: 0.20118889212608337 epoch 21 batch 20 \n",
            "total loss: 0.19008702039718628 epoch 21 batch 25 \n",
            "total loss: 0.20112887024879456 epoch 21 batch 30 \n",
            "total loss: 0.20319883525371552 epoch 21 batch 35 \n",
            "total loss: 0.2021166980266571 epoch 21 batch 40 \n",
            "total loss: 0.1964733749628067 epoch 21 batch 45 \n",
            "total loss: 0.2138829231262207 epoch 21 batch 50 \n",
            "total loss: 0.22604326903820038 epoch 21 batch 55 \n",
            "total loss: 0.23117975890636444 epoch 21 batch 60 \n",
            "total loss: 0.22160311043262482 epoch 21 batch 65 \n",
            "total loss: 0.2113226354122162 epoch 21 batch 70 \n",
            "total loss: 0.22002002596855164 epoch 21 batch 75 \n",
            "total loss: 0.16174589097499847 epoch 22 batch 5 \n",
            "total loss: 0.1733194887638092 epoch 22 batch 10 \n",
            "total loss: 0.1708705723285675 epoch 22 batch 15 \n",
            "total loss: 0.15747319161891937 epoch 22 batch 20 \n",
            "total loss: 0.19251613318920135 epoch 22 batch 25 \n",
            "total loss: 0.16249847412109375 epoch 22 batch 30 \n",
            "total loss: 0.19472230970859528 epoch 22 batch 35 \n",
            "total loss: 0.190814808011055 epoch 22 batch 40 \n",
            "total loss: 0.19092142581939697 epoch 22 batch 45 \n",
            "total loss: 0.17998577654361725 epoch 22 batch 50 \n",
            "total loss: 0.17127889394760132 epoch 22 batch 55 \n",
            "total loss: 0.20568221807479858 epoch 22 batch 60 \n",
            "total loss: 0.19607777893543243 epoch 22 batch 65 \n",
            "total loss: 0.18879638612270355 epoch 22 batch 70 \n",
            "total loss: 0.21133270859718323 epoch 22 batch 75 \n",
            "total loss: 0.14818160235881805 epoch 23 batch 5 \n",
            "total loss: 0.13921736180782318 epoch 23 batch 10 \n",
            "total loss: 0.14862613379955292 epoch 23 batch 15 \n",
            "total loss: 0.15794169902801514 epoch 23 batch 20 \n",
            "total loss: 0.1585460603237152 epoch 23 batch 25 \n",
            "total loss: 0.15236099064350128 epoch 23 batch 30 \n",
            "total loss: 0.14388026297092438 epoch 23 batch 35 \n",
            "total loss: 0.1730482578277588 epoch 23 batch 40 \n",
            "total loss: 0.16559867560863495 epoch 23 batch 45 \n",
            "total loss: 0.1592603325843811 epoch 23 batch 50 \n",
            "total loss: 0.17909321188926697 epoch 23 batch 55 \n",
            "total loss: 0.1688736081123352 epoch 23 batch 60 \n",
            "total loss: 0.18387563526630402 epoch 23 batch 65 \n",
            "total loss: 0.20079074800014496 epoch 23 batch 70 \n",
            "total loss: 0.18479610979557037 epoch 23 batch 75 \n",
            "total loss: 0.12584497034549713 epoch 24 batch 5 \n",
            "total loss: 0.16827158629894257 epoch 24 batch 10 \n",
            "total loss: 0.14708538353443146 epoch 24 batch 15 \n",
            "total loss: 0.14080186188220978 epoch 24 batch 20 \n",
            "total loss: 0.15004460513591766 epoch 24 batch 25 \n",
            "total loss: 0.15724214911460876 epoch 24 batch 30 \n",
            "total loss: 0.1450737863779068 epoch 24 batch 40 \n",
            "total loss: 0.1436743289232254 epoch 24 batch 45 \n",
            "total loss: 0.16658304631710052 epoch 24 batch 50 \n",
            "total loss: 0.15421901643276215 epoch 24 batch 55 \n",
            "total loss: 0.14567694067955017 epoch 24 batch 60 \n",
            "total loss: 0.15692807734012604 epoch 24 batch 65 \n",
            "total loss: 0.16913823783397675 epoch 24 batch 70 \n",
            "total loss: 0.16396887600421906 epoch 24 batch 75 \n",
            "total loss: 0.12310551106929779 epoch 25 batch 5 \n",
            "total loss: 0.1231217235326767 epoch 25 batch 10 \n",
            "total loss: 0.13461834192276 epoch 25 batch 15 \n",
            "total loss: 0.13740701973438263 epoch 25 batch 20 \n",
            "total loss: 0.1461639553308487 epoch 25 batch 25 \n",
            "total loss: 0.12763641774654388 epoch 25 batch 30 \n",
            "total loss: 0.1298750787973404 epoch 25 batch 35 \n",
            "total loss: 0.14035964012145996 epoch 25 batch 40 \n",
            "total loss: 0.1380896270275116 epoch 25 batch 45 \n",
            "total loss: 0.15095414221286774 epoch 25 batch 50 \n",
            "total loss: 0.15059277415275574 epoch 25 batch 55 \n",
            "total loss: 0.1374197006225586 epoch 25 batch 60 \n",
            "total loss: 0.1753493696451187 epoch 25 batch 65 \n",
            "total loss: 0.17539168894290924 epoch 25 batch 70 \n",
            "total loss: 0.1967940330505371 epoch 25 batch 75 \n",
            "total loss: 0.13913415372371674 epoch 26 batch 5 \n",
            "total loss: 0.1260342001914978 epoch 26 batch 10 \n",
            "total loss: 0.12975582480430603 epoch 26 batch 15 \n",
            "total loss: 0.12724651396274567 epoch 26 batch 20 \n",
            "total loss: 0.1290447860956192 epoch 26 batch 25 \n",
            "total loss: 0.1299382746219635 epoch 26 batch 30 \n",
            "total loss: 0.14393271505832672 epoch 26 batch 35 \n",
            "total loss: 0.1513812094926834 epoch 26 batch 40 \n",
            "total loss: 0.1535494178533554 epoch 26 batch 45 \n",
            "total loss: 0.15854749083518982 epoch 26 batch 50 \n",
            "total loss: 0.1471175253391266 epoch 26 batch 55 \n",
            "total loss: 0.15259233117103577 epoch 26 batch 60 \n",
            "total loss: 0.158135786652565 epoch 26 batch 65 \n",
            "total loss: 0.15608292818069458 epoch 26 batch 70 \n",
            "total loss: 0.16399520635604858 epoch 26 batch 75 \n",
            "total loss: 0.11544706672430038 epoch 27 batch 5 \n",
            "total loss: 0.13643141090869904 epoch 27 batch 10 \n",
            "total loss: 0.126165509223938 epoch 27 batch 15 \n",
            "total loss: 0.14374545216560364 epoch 27 batch 20 \n",
            "total loss: 0.1368081122636795 epoch 27 batch 25 \n",
            "total loss: 0.12535761296749115 epoch 27 batch 30 \n",
            "total loss: 0.1346099078655243 epoch 27 batch 35 \n",
            "total loss: 0.1364043951034546 epoch 27 batch 40 \n",
            "total loss: 0.15424667298793793 epoch 27 batch 45 \n",
            "total loss: 0.13971176743507385 epoch 27 batch 50 \n",
            "total loss: 0.14598634839057922 epoch 27 batch 55 \n",
            "total loss: 0.1526559740304947 epoch 27 batch 60 \n",
            "total loss: 0.15743869543075562 epoch 27 batch 65 \n",
            "total loss: 0.1553059071302414 epoch 27 batch 70 \n",
            "total loss: 0.1494155079126358 epoch 27 batch 75 \n",
            "total loss: 0.12898439168930054 epoch 28 batch 5 \n",
            "total loss: 0.10839365422725677 epoch 28 batch 10 \n",
            "total loss: 0.14011932909488678 epoch 28 batch 15 \n",
            "total loss: 0.12003157287836075 epoch 28 batch 20 \n",
            "total loss: 0.12830838561058044 epoch 28 batch 25 \n",
            "total loss: 0.13705429434776306 epoch 28 batch 30 \n",
            "total loss: 0.11849915236234665 epoch 28 batch 35 \n",
            "total loss: 0.12029840797185898 epoch 28 batch 40 \n",
            "total loss: 0.12581148743629456 epoch 28 batch 45 \n",
            "total loss: 0.12891018390655518 epoch 28 batch 50 \n",
            "total loss: 0.14587542414665222 epoch 28 batch 55 \n",
            "total loss: 0.12467249482870102 epoch 28 batch 60 \n",
            "total loss: 0.15904581546783447 epoch 28 batch 65 \n",
            "total loss: 0.14014200866222382 epoch 28 batch 70 \n",
            "total loss: 0.13153545558452606 epoch 28 batch 75 \n",
            "total loss: 0.11272622644901276 epoch 29 batch 5 \n",
            "total loss: 0.10764838755130768 epoch 29 batch 10 \n",
            "total loss: 0.10196951031684875 epoch 29 batch 15 \n",
            "total loss: 0.1252071112394333 epoch 29 batch 20 \n",
            "total loss: 0.12701918184757233 epoch 29 batch 25 \n",
            "total loss: 0.1169179156422615 epoch 29 batch 30 \n",
            "total loss: 0.11338238418102264 epoch 29 batch 35 \n",
            "total loss: 0.1135607585310936 epoch 29 batch 40 \n",
            "total loss: 0.11161436885595322 epoch 29 batch 45 \n",
            "total loss: 0.12990744411945343 epoch 29 batch 50 \n",
            "total loss: 0.11997058987617493 epoch 29 batch 55 \n",
            "total loss: 0.11104953289031982 epoch 29 batch 60 \n",
            "total loss: 0.12385718524456024 epoch 29 batch 65 \n",
            "total loss: 0.13310398161411285 epoch 29 batch 70 \n",
            "total loss: 0.12063929438591003 epoch 29 batch 75 \n",
            "total loss: 0.08709969371557236 epoch 30 batch 5 \n",
            "total loss: 0.0871819406747818 epoch 30 batch 10 \n",
            "total loss: 0.08952832221984863 epoch 30 batch 15 \n",
            "total loss: 0.08407390862703323 epoch 30 batch 20 \n",
            "total loss: 0.09261499345302582 epoch 30 batch 25 \n",
            "total loss: 0.08896800875663757 epoch 30 batch 30 \n",
            "total loss: 0.08707350492477417 epoch 30 batch 35 \n",
            "total loss: 0.09367158263921738 epoch 30 batch 40 \n",
            "total loss: 0.09785870462656021 epoch 30 batch 45 \n",
            "total loss: 0.10647179931402206 epoch 30 batch 50 \n",
            "total loss: 0.10551503300666809 epoch 30 batch 55 \n",
            "total loss: 0.0988578349351883 epoch 30 batch 60 \n",
            "total loss: 0.0970694050192833 epoch 30 batch 65 \n",
            "total loss: 0.11183032393455505 epoch 30 batch 70 \n",
            "total loss: 0.11022362858057022 epoch 30 batch 75 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDyK-EGqbN5r",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y98sfom7SuGy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(input_sequences):\n",
        "    #In this section we evaluate our model on a raw_input converted to german, for this the entire sentence has to be passed\n",
        "    #through the length of the model, for this we use greedsampler to run through the decoder\n",
        "    #and the final embedding matrix trained on the data is used to generate embeddings\n",
        "    # input_raw='▁i ▁agree ▁that ▁we ▁need ▁an ▁ambitious ▁social ▁agenda ▁which ▁will ▁include ▁combating ▁poverty ▁and ▁social ▁exclusion'\n",
        "    input_sequences = tf.keras.preprocessing.sequence.pad_sequences(input_sequences,\n",
        "                                                                    maxlen=Tx, padding='post')\n",
        "    inp = tf.convert_to_tensor(input_sequences)\n",
        "    # print(inp.shape)\n",
        "    inference_batch_size = input_sequences.shape[0]\n",
        "    encoder_initial_cell_state = [tf.zeros((inference_batch_size, rnn_units)),\n",
        "                                tf.zeros((inference_batch_size, rnn_units))]\n",
        "    # encoder_emb_inp = encoderNetwork.encoder_embedding(inp)\n",
        "    # a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp,\n",
        "    #                                                 initial_state =encoder_initial_cell_state)\n",
        "    a, a_tx, c_tx = encoderNetwork(inp,encoder_initial_cell_state)\n",
        "    # print('a_tx :',a_tx.shape)\n",
        "    # print('c_tx :', c_tx.shape)\n",
        "\n",
        "    start_tokens = tf.fill([inference_batch_size], 1)\n",
        "\n",
        "    end_token = 2\n",
        "\n",
        "    greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
        "\n",
        "    decoder_input = tf.expand_dims([1]* inference_batch_size,1)\n",
        "    decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
        "\n",
        "    decoder_instance = tfa.seq2seq.BasicDecoder(cell = decoderNetwork.rnn_cell, sampler = greedy_sampler,\n",
        "                                                output_layer=decoderNetwork.dense_layer)\n",
        "    decoderNetwork.attention_mechanism.setup_memory(a)\n",
        "    #pass [ last step activations , encoder memory_state ] as input to decoder for LSTM\n",
        "    # print(\"decoder_initial_state = [a_tx, c_tx] :\",np.array([a_tx, c_tx]).shape)\n",
        "    decoder_initial_state = decoderNetwork.build_decoder_initial_state(inference_batch_size,\n",
        "                                                                    encoder_state=[a_tx, c_tx],\n",
        "                                                                    Dtype=tf.float32)\n",
        "    # print(\"\\nCompared to simple encoder-decoder without attention, the decoder_initial_state \\\n",
        "    #  is an AttentionWrapperState object containing s_prev tensors and context and alignment vector \\n \")\n",
        "    # print(\"decoder initial state shape :\",np.array(decoder_initial_state).shape)\n",
        "    # print(\"decoder_initial_state tensor \\n\", decoder_initial_state)\n",
        "\n",
        "    # Since we do not know the target sequence lengths in advance, we use maximum_iterations to limit the translation lengths.\n",
        "    # One heuristic is to decode up to two times the source sentence lengths.\n",
        "    maximum_iterations = tf.round(tf.reduce_max(Tx) * 2)\n",
        "\n",
        "    #initialize inference decoder\n",
        "    decoder_embedding_matrix = decoderNetwork.decoder_embedding.variables[0] \n",
        "    (first_finished, first_inputs,first_state) = decoder_instance.initialize(decoder_embedding_matrix,\n",
        "                                start_tokens = start_tokens,\n",
        "                                end_token=end_token,\n",
        "                                initial_state = decoder_initial_state)\n",
        "    #print( first_finished.shape)\n",
        "    # print(\"\\nfirst_inputs returns the same decoder_input i.e. embedding of  <start> :\",first_inputs.shape)\n",
        "    # print(\"start_index_emb_avg \", tf.reduce_sum(tf.reduce_mean(first_inputs, axis=0))) # mean along the batch\n",
        "\n",
        "    inputs = first_inputs\n",
        "    state = first_state  \n",
        "    predictions = np.empty((inference_batch_size,0), dtype = np.int32)                                                                             \n",
        "    for j in range(maximum_iterations):\n",
        "        outputs, next_state, next_inputs, finished = decoder_instance.step(j,inputs,state)\n",
        "        inputs = next_inputs\n",
        "        state = next_state\n",
        "        outputs = np.expand_dims(outputs.sample_id,axis = -1)\n",
        "        predictions = np.append(predictions, outputs, axis = -1)\n",
        "    return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iodjSItQds1t",
        "colab_type": "text"
      },
      "source": [
        "## Final Translation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dj7IWcGpcbat",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_data_en=load_data(\"sub_test.lang1.atok\", end_tok=False)\n",
        "test_data_en=load_ids(sp_en, \"sub_test.lang1\")\n",
        "predictions = predict(test_data_en)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wqNvmqvA5bK",
        "colab_type": "code",
        "outputId": "795717a6-b0b2-4815-a2c6-21687e438858",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "line_ = list(itertools.takewhile( lambda index: index !=2, predictions[10].tolist()))\n",
        "print(sp_en.decode_ids(test_data_en[10]))\n",
        "print(sp_fr.decode_ids(line_))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i did n't sleep well\n",
            "J ' ai fait sans faire .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6aWFB5IWlH2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#prediction based on our sentence earlier\n",
        "with open(\"sentencepiece_seq2seq_test_fr_pred.txt\", 'w') as f_out:\n",
        "    for i in range(len(predictions)):\n",
        "        line = predictions[i,:].tolist()\n",
        "        seq = list(itertools.takewhile( lambda index: index !=2, line))\n",
        "        f_out.writelines([sp_fr.decode_ids(seq), '\\n'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56gs08WRyDV9",
        "colab_type": "text"
      },
      "source": [
        "## bleu score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-flnZ27x6Nh",
        "colab_type": "code",
        "outputId": "9630fc0e-57ce-406d-98f3-504196790d22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!perl multi-bleu.perl sub_test.lang2 < sentencepiece_seq2seq_test_fr_pred.txt"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU = 9.19, 36.5/12.5/5.7/2.8 (BP=1.000, ratio=1.024, hyp_len=25377, ref_len=24780)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6Av-oPWvRc4",
        "colab_type": "text"
      },
      "source": [
        "### The accuracy can be improved by implementing:\n",
        "* Beam Search or Lexicon Search\n",
        "* Bi-directional encoder-decoder model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pI9cel0_AdNM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_beam(input_sequences):       \n",
        "\n",
        "    \"\"\"### Inference using Beam Search with beam_width = 3\"\"\"\n",
        "\n",
        "    beam_width = 3\n",
        "    input_sequences = tf.keras.preprocessing.sequence.pad_sequences(input_sequences,\n",
        "                                                                    maxlen=Tx, padding='post')\n",
        "    inp = tf.convert_to_tensor(input_sequences)\n",
        "    #print(inp.shape)\n",
        "    inference_batch_size = input_sequences.shape[0]\n",
        "    encoder_initial_cell_state = [tf.zeros((inference_batch_size, rnn_units)),\n",
        "                                tf.zeros((inference_batch_size, rnn_units))]\n",
        "    a, a_tx, c_tx = encoderNetwork(inp,encoder_initial_cell_state)\n",
        "    #pass [ last step activations , encoder memory_state ] as input to decoder for LSTM\n",
        "    s_prev = [a_tx, c_tx]\n",
        "\n",
        "    start_tokens = tf.fill([inference_batch_size],1)\n",
        "    end_token = 2\n",
        "\n",
        "    decoder_input = tf.expand_dims([1]* inference_batch_size,1)\n",
        "    decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
        "\n",
        "\n",
        "    #From official documentation\n",
        "    #NOTE If you are using the BeamSearchDecoder with a cell wrapped in AttentionWrapper, then you must ensure that:\n",
        "\n",
        "    #The encoder output has been tiled to beam_width via tfa.seq2seq.tile_batch (NOT tf.tile).\n",
        "    #The batch_size argument passed to the get_initial_state method of this wrapper is equal to true_batch_size * beam_width.\n",
        "    #The initial state created with get_initial_state above contains a cell_state value containing properly tiled final state from the encoder.\n",
        "    encoder_memory = tfa.seq2seq.tile_batch(a, beam_width)\n",
        "    decoderNetwork.attention_mechanism.setup_memory(encoder_memory)\n",
        "    print(\"beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] :\", encoder_memory.shape)\n",
        "    #set decoder_inital_state which is an AttentionWrapperState considering beam_width\n",
        "    decoder_initial_state = decoderNetwork.rnn_cell.get_initial_state(batch_size = inference_batch_size* beam_width,dtype = Dtype)\n",
        "    encoder_state = tfa.seq2seq.tile_batch(s_prev, multiplier=beam_width)\n",
        "    decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state) \n",
        "\n",
        "    decoder_instance = tfa.seq2seq.BeamSearchDecoder(decoderNetwork.rnn_cell,beam_width=beam_width,\n",
        "                                                    output_layer=decoderNetwork.dense_layer)\n",
        "\n",
        "\n",
        "    # Since we do not know the target sequence lengths in advance, we use maximum_iterations to limit the translation lengths.\n",
        "    # One heuristic is to decode up to two times the source sentence lengths.\n",
        "    maximum_iterations = tf.round(tf.reduce_max(Tx) * 2)\n",
        "\n",
        "    #initialize inference decoder\n",
        "    decoder_embedding_matrix = decoderNetwork.decoder_embedding.variables[0] \n",
        "    (first_finished, first_inputs,first_state) = decoder_instance.initialize(decoder_embedding_matrix,\n",
        "                                start_tokens = start_tokens,\n",
        "                                end_token=end_token,\n",
        "                                initial_state = decoder_initial_state)\n",
        "    #print( first_finished.shape)\n",
        "    print(\"\\nfirst_inputs returns the same decoder_input i.e. embedding of  <start> :\",first_inputs.shape)\n",
        "\n",
        "    inputs = first_inputs\n",
        "    state = first_state  \n",
        "    predictions = np.empty((inference_batch_size, beam_width,0), dtype = np.int32)\n",
        "    beam_scores =  np.empty((inference_batch_size, beam_width,0), dtype = np.float32)                                                                            \n",
        "    for j in range(maximum_iterations):\n",
        "        beam_search_outputs, next_state, next_inputs, finished = decoder_instance.step(j,inputs,state)\n",
        "        inputs = next_inputs\n",
        "        state = next_state\n",
        "        outputs = np.expand_dims(beam_search_outputs.predicted_ids,axis = -1)\n",
        "        scores = np.expand_dims(beam_search_outputs.scores,axis = -1)\n",
        "        predictions = np.append(predictions, outputs, axis = -1)\n",
        "        beam_scores = np.append(beam_scores, scores, axis = -1)\n",
        "    print(predictions.shape) \n",
        "    print(beam_scores.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}